# =========================================
# DigiCow Challenge - Complete Final Submission Pipeline
# =========================================

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import roc_auc_score, log_loss, brier_score_loss, roc_curve
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.calibration import CalibratedClassifierCV, calibration_curve
import lightgbm as lgb
from lightgbm import LGBMClassifier
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Set matplotlib backend to non-interactive
import matplotlib
matplotlib.use('Agg')

print("=" * 80)
print("DIGICOW CHALLENGE - COMPLETE FINAL SUBMISSION PIPELINE")
print("=" * 80)

# ------------------------
# 1Ô∏è‚É£ Load and Prepare Data
# ------------------------
print("\nüìÇ 1Ô∏è‚É£ LOADING AND PREPARING DATA:")

# Load raw data
train = pd.read_csv("data/Train.csv")
test = pd.read_csv("data/Test.csv")

print(f"Training data shape: {train.shape}")
print(f"Test data shape: {test.shape}")

target_col = 'adopted_within_07_days'
print(f"Target variable: {target_col}")

# ------------------------
# 2Ô∏è‚É£ Advanced Feature Engineering
# ------------------------
print("\nüîß 2Ô∏è‚É£ ADVANCED FEATURE ENGINEERING:")

def engineer_features(df, is_train=True):
    """Comprehensive feature engineering"""
    df = df.copy()
    
    # Topics count feature
    df['num_topics'] = df['topics_list'].apply(lambda x: len(str(x).split(',')) if pd.notnull(x) else 0)
    
    # Date features
    df['first_training_date'] = pd.to_datetime(df['first_training_date'])
    df['train_dayofweek'] = df['first_training_date'].dt.dayofweek
    df['train_month'] = df['first_training_date'].dt.month
    df['train_is_weekend'] = (df['train_dayofweek'] >= 5).astype(int)
    df['train_quarter'] = df['first_training_date'].dt.quarter
    df['train_dayofyear'] = df['first_training_date'].dt.dayofyear
    
    # Handle missing values
    df['days_to_second_training'] = df['days_to_second_training'].fillna(999)
    df['has_second_training'] = df['has_second_training'].fillna(0)
    
    # Numeric features - fill with median
    numeric_cols = ['belong_to_cooperative', 'num_trainings_30d', 'num_trainings_60d', 
                   'num_total_trainings', 'num_repeat_trainings', 'num_unique_trainers']
    for col in numeric_cols:
        if col in df.columns:
            df[col] = df[col].fillna(df[col].median())
    
    # Categorical features - fill with 'Unknown'
    categorical_cols = ['gender', 'age', 'registration', 'county', 'subcounty', 'ward', 'trainer']
    for col in categorical_cols:
        if col in df.columns:
            df[col] = df[col].fillna('Unknown')
    
    # Target encoding for high-cardinality features (only for training)
    if is_train and target_col in df.columns:
        # Trainer encoding
        trainer_mean = df.groupby('trainer')[target_col].mean()
        overall_mean = df[target_col].mean()
        df['trainer_encoded'] = df['trainer'].map(trainer_mean).fillna(overall_mean)
        
        # County encoding
        county_mean = df.groupby('county')[target_col].mean()
        df['county_encoded'] = df['county'].map(county_mean).fillna(overall_mean)
        
        # Store mappings for test set
        global trainer_mapping, county_mapping, global_mean
        trainer_mapping = trainer_mean
        county_mapping = county_mean
        global_mean = overall_mean
    
    # Interaction features
    df['training_intensity'] = df['num_trainings_30d'] * df['num_trainings_60d']
    df['training_frequency'] = df['num_total_trainings'] / (df['days_to_second_training'] + 1)
    df['training_month_intensity'] = df['train_month'] * df['num_trainings_30d']
    df['weekend_training_effect'] = df['train_is_weekend'] * df['num_trainings_30d']
    
    # Age binning
    age_bins = {'Below 35': 0, '35-50': 1, 'Above 50': 2}
    df['age_numeric'] = df['age'].map(age_bins)
    
    return df

# Engineer features
train_engineered = engineer_features(train, is_train=True)
test_engineered = engineer_features(test, is_train=False)

# Apply target encoding to test set
test_engineered['trainer_encoded'] = test_engineered['trainer'].map(trainer_mapping).fillna(global_mean)
test_engineered['county_encoded'] = test_engineered['county'].map(county_mapping).fillna(global_mean)

print("‚úÖ Feature engineering completed")

# ------------------------
# 3Ô∏è‚É£ Feature Selection and Encoding
# ------------------------
print("\nüéØ 3Ô∏è‚É£ FEATURE SELECTION AND ENCODING:")

# Select features for modeling
exclude_cols = ['ID', 'first_training_date', 'topics_list', 'county', 'subcounty', 'ward', 
                'trainer', target_col]

# Identify categorical features for one-hot encoding
categorical_features = ['gender', 'age', 'registration']
numerical_features = [
    'belong_to_cooperative', 'num_trainings_30d', 'num_trainings_60d',
    'num_total_trainings', 'num_repeat_trainings', 'days_to_second_training',
    'num_unique_trainers', 'has_second_training', 'num_topics',
    'train_dayofweek', 'train_month', 'train_is_weekend', 'train_quarter',
    'train_dayofyear', 'trainer_encoded', 'county_encoded',
    'training_intensity', 'training_frequency', 'training_month_intensity',
    'weekend_training_effect', 'age_numeric'
]

# One-hot encode categorical features
train_encoded = pd.get_dummies(train_engineered, columns=categorical_features, drop_first=True)
test_encoded = pd.get_dummies(test_engineered, columns=categorical_features, drop_first=True)

# Ensure test has same columns as train
for col in train_encoded.columns:
    if col not in test_encoded.columns and col != target_col:
        test_encoded[col] = 0

# Align columns
feature_cols = [col for col in train_encoded.columns if col not in exclude_cols + [target_col]]
test_encoded = test_encoded[feature_cols]

print(f"Selected {len(feature_cols)} features for modeling")

# Save feature list
with open("feature_list.txt", "w") as f:
    for feature in feature_cols:
        f.write(f"{feature}\n")

print("‚úÖ Feature list saved to feature_list.txt")

# ------------------------
# 4Ô∏è‚É£ Train-Validation Split
# ------------------------
print("\nüîÑ 4Ô∏è‚É£ TRAIN-VALIDATION SPLIT:")

X = train_encoded[feature_cols]
y = train_encoded[target_col]

X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"Training set: {X_train.shape}")
print(f"Validation set: {X_val.shape}")
print(f"Training adoption rate: {y_train.mean():.3f}")
print(f"Validation adoption rate: {y_val.mean():.3f}")

# ------------------------
# 5Ô∏è‚É£ Model Training with Class Imbalance Handling
# ------------------------
print("\nüå≥ 5Ô∏è‚É£ MODEL TRAINING:")

# Calculate class weights
class_weight_ratio = (len(y_train) - sum(y_train)) / sum(y_train)
print(f"Class weight ratio (positive/negative): {class_weight_ratio:.2f}")

# Initialize LightGBM with optimal parameters
model = LGBMClassifier(
    objective='binary',
    boosting_type='gbdt',
    learning_rate=0.03,
    num_leaves=64,
    max_depth=-1,
    min_child_samples=10,
    n_estimators=2000,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_alpha=0.1,
    reg_lambda=0.1,
    random_state=42,
    verbose=-1,
    class_weight=None,  # Removed to avoid confusion with scale_pos_weight
    scale_pos_weight=class_weight_ratio  # Recommended for LogLoss
)

# Train with early stopping
model.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    eval_metric='auc',
    callbacks=[lgb.early_stopping(100), lgb.log_evaluation(100)]
)

print("‚úÖ Model training completed")

# ------------------------
# 6Ô∏è‚É£ Initial Model Evaluation
# ------------------------
print("\nüìä 6Ô∏è‚É£ INITIAL MODEL EVALUATION:")

# Predict probabilities
y_train_pred = model.predict_proba(X_train)[:, 1]
y_val_pred = model.predict_proba(X_val)[:, 1]

# Calculate metrics
train_auc = roc_auc_score(y_train, y_train_pred)
val_auc = roc_auc_score(y_val, y_val_pred)
train_logloss = log_loss(y_train, y_train_pred)
val_logloss = log_loss(y_val, y_val_pred)

print(f"Training ROC-AUC: {train_auc:.4f}")
print(f"Validation ROC-AUC: {val_auc:.4f}")
print(f"Training Log Loss: {train_logloss:.4f}")
print(f"Validation Log Loss: {val_logloss:.4f}")

# ------------------------
# 7Ô∏è‚É£ Probability Calibration (Leakage-Free)
# ------------------------
print("\nüîß 7Ô∏è‚É£ PROBABILITY CALIBRATION (LEAKAGE-FREE):")

# Try multiple calibration methods using validation set only (no CV leakage)
calibration_methods = {
    'Original': None,
    'Platt Scaling': 'sigmoid',
    'Isotonic Regression': 'isotonic'
}

calibrated_models = {}
calibration_results = []

for method_name, method in calibration_methods.items():
    if method is None:
        # Original model
        calibrated_models[method_name] = model
        val_pred_calibrated = y_val_pred
        test_pred_calibrated = model.predict_proba(test_encoded)[:, 1]
    else:
        # Calibrated model using prefit and validation set only (no leakage)
        calibrated = CalibratedClassifierCV(
            model,
            method=method,
            cv=None  # Use None for prefit mode
        )
        calibrated.fit(X_val, y_val)  # Only use validation set
        calibrated_models[method_name] = calibrated
        
        val_pred_calibrated = calibrated.predict_proba(X_val)[:, 1]
        test_pred_calibrated = calibrated.predict_proba(test_encoded)[:, 1]
    
    # Calculate metrics
    auc = roc_auc_score(y_val, val_pred_calibrated)
    logloss = log_loss(y_val, val_pred_calibrated)
    brier = brier_score_loss(y_val, val_pred_calibrated)
    
    calibration_results.append({
        'Method': method_name,
        'ROC-AUC': auc,
        'Log Loss': logloss,
        'Brier Score': brier,
        'Test_Predictions': test_pred_calibrated
    })
    
    print(f"{method_name}: AUC={auc:.4f}, LogLoss={logloss:.4f}, Brier={brier:.4f}")

# Find best calibration method
calibration_df = pd.DataFrame(calibration_results)
best_method = calibration_df.loc[calibration_df['Log Loss'].idxmin(), 'Method']
best_logloss = calibration_df['Log Loss'].min()

print(f"\nüèÜ Best calibration method: {best_method} (Log Loss: {best_logloss:.4f})")

# ------------------------
# 8Ô∏è‚É£ Visualization
# ------------------------
print("\nüìà 8Ô∏è‚É£ CREATING VISUALIZATIONS:")

# 8.1 ROC Curve
plt.figure(figsize=(10, 8))
fpr, tpr, _ = roc_curve(y_val, y_val_pred)
plt.plot(fpr, tpr, linewidth=2, label=f'LightGBM (AUC = {val_auc:.4f})')
plt.plot([0, 1], [0, 1], '--', color='gray', label='Random Classifier')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.grid(True, alpha=0.3)
plt.savefig('roc_curve.png', dpi=300, bbox_inches='tight')
plt.close()

# 8.2 Feature Importance
feature_importance = pd.DataFrame({
    'feature': feature_cols,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

plt.figure(figsize=(12, 8))
top_features = feature_importance.head(15)
sns.barplot(data=top_features, x='importance', y='feature')
plt.title('Top 15 Feature Importance')
plt.xlabel('Importance Score')
plt.tight_layout()
plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')
plt.close()

# 8.3 Probability Distributions
plt.figure(figsize=(12, 6))
plt.hist(y_val_pred[y_val == 0], bins=50, alpha=0.7, label='Non-Adopters', color='blue', density=True)
plt.hist(y_val_pred[y_val == 1], bins=50, alpha=0.7, label='Adopters', color='red', density=True)
plt.xlabel('Predicted Probability')
plt.ylabel('Density')
plt.title('Probability Distributions: Adopters vs Non-Adopters')
plt.legend()
plt.grid(True, alpha=0.3)
plt.savefig('probability_distributions.png', dpi=300, bbox_inches='tight')
plt.close()

# 8.4 Calibration Curves
fig, axes = plt.subplots(2, 2, figsize=(15, 12))
fig.suptitle('Calibration Curves Comparison', fontsize=16, fontweight='bold')

for i, (_, result) in enumerate(calibration_df.iterrows()):
    if i >= 4:  # Only show first 4 methods
        break
    
    ax = axes[i // 2, i % 2]
    
    if result['Method'] == 'Original':
        prob_true, prob_pred = calibration_curve(y_val, y_val_pred, n_bins=10)
    else:
        val_pred = calibrated_models[result['Method']].predict_proba(X_val)[:, 1]
        prob_true, prob_pred = calibration_curve(y_val, val_pred, n_bins=10)
    
    ax.plot(prob_pred, prob_true, marker='o', linewidth=2, label=result['Method'])
    ax.plot([0, 1], [0, 1], '--', color='gray', label='Perfectly Calibrated')
    ax.set_title(f'{result["Method"]} (Log Loss: {result["Log Loss"]:.4f})')
    ax.set_xlabel('Mean Predicted Probability')
    ax.set_ylabel('Fraction of Positives')
    ax.legend()
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('calibration_curves.png', dpi=300, bbox_inches='tight')
plt.close()

print("‚úÖ All visualizations saved")

# ------------------------
# 9Ô∏è‚É£ Generate Final Submission (Zindi-compliant)
# ------------------------

print("\nüîÆ 9Ô∏è‚É£ GENERATING FINAL SUBMISSION:")

test_original = pd.read_csv("data/Test.csv")

# Use best calibrated predictions
best_result = calibration_df.loc[calibration_df['Log Loss'].idxmin()]
best_predictions = best_result['Test_Predictions']

# Apply epsilon clipping to improve LogLoss
eps = 1e-6
best_predictions = np.clip(best_predictions, eps, 1 - eps)

submission = pd.DataFrame({
    'ID': test_original['ID'],
    'Target': best_predictions
})

submission.to_csv("submission_final.csv", index=False)

print("‚úÖ Created submission_final.csv (READY FOR ZINDI)")

# ------------------------
# üîü Final Summary
# ------------------------
print("\n" + "=" * 80)
print("üéØ FINAL SUBMISSION PIPELINE SUMMARY")
print("=" * 80)

print(f"\nüìä MODEL PERFORMANCE:")
print(f"Validation ROC-AUC: {val_auc:.4f}")
print(f"Validation Log Loss: {val_logloss:.4f}")
print(f"Best Calibrated Log Loss: {best_logloss:.4f}")
print(f"Log Loss Improvement: {val_logloss - best_logloss:.4f}")

print(f"\nüèÜ TOP 5 FEATURES:")
for i, row in feature_importance.head(5).iterrows():
    print(f"{i+1}. {row['feature']}: {row['importance']:.0f}")

print(f"\nüìà CALIBRATION RESULTS:")
for _, row in calibration_df.iterrows():
    print(f"{row['Method']}: AUC={row['ROC-AUC']:.4f}, LogLoss={row['Log Loss']:.4f}")

print(f"\nüíæ SUBMISSION FILE CREATED:")
print(f"- submission_final.csv (ZINDI-COMPLIANT)")

print(f"\nüìä VISUALIZATION FILES:")
print(f"- roc_curve.png")
print(f"- feature_importance.png")
print(f"- probability_distributions.png")
print(f"- calibration_curves.png")

print(f"\nüéØ LEADERBOARD METRICS:")
print(f"Log Loss (70% weight): {best_logloss:.4f}")
print(f"ROC-AUC (30% weight): {calibration_df.loc[calibration_df['Log Loss'].idxmin(), 'ROC-AUC']:.4f}")

print(f"\n‚úÖ Leakage-free pipeline finished! Ready for Zindi submission!")
print("=" * 80)
